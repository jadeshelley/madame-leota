# Madame Leota Mouth Animation System - Technical Status & Options

## Current Working Status âœ…

As of December 2024, the face animation system is **working** with the following achievements:

### What's Working:
- âœ… **Face loading**: Successfully loads custom face images from `assets/faces/`
- âœ… **Display system**: Full face visible, properly centered, no black boxes
- âœ… **Facial landmark detection**: dlib successfully detects 68 facial landmarks
- âœ… **Audio analysis**: Real-time amplitude and frequency analysis working
- âœ… **Mouth movement**: Visible mouth animation synchronized to speech
- âœ… **Safety systems**: Prevents corruption, handles errors gracefully

### Key Fixes Applied:
1. **Face cropping**: Fixed from 600Ã—500 to 1400Ã—1000 pixel crop (shows full face)
2. **Black box elimination**: Added safety checks to prevent corrupted mouth warping
3. **Enhanced movement intensity**: Dramatically increased jaw drop, lip width/height ranges
4. **Audio sensitivity**: 2x boost in amplitude detection
5. **Screen positioning**: Proper centering, no cutoffs

---

## Current Issue: "Defined Block" Around Mouth ğŸ”§

### The Problem:
- Visible rectangular boundary around mouth during animation
- Mouth region looks "cut out" and blended back in
- Not seamless like real AI-generated lip sync

### Technical Cause:
```python
# Current approach in src/dlib_face_animator.py:
1. Extract rectangular mouth region: mouth_region = face[y:y+h, x:x+w]
2. Transform the rectangle: warped_region = self._apply_triangular_warp(...)
3. Blend back: result[y:y+h, x:x+w] = blended_region
```

The rectangular extraction/blending creates visible boundaries.

---

## Animation Approach Analysis

### Current System: Traditional Computer Vision (dlib + OpenCV)

**Method**: 
- dlib detects 68 facial landmarks
- Extract mouth region (rectangular)
- Apply geometric transformations (scaling, warping)
- Blend transformed region back into face

**Pros**:
- âœ… Lightweight, runs on Raspberry Pi
- âœ… No additional model downloads
- âœ… Real-time performance
- âœ… Currently working

**Cons**:
- âŒ Visible boundaries around mouth
- âŒ Limited realism (geometric transforms only)
- âŒ Rectangle-based approach creates artifacts

**Current Parameters** (in `src/dlib_face_animator.py`):
```python
jaw_drop = amplitude * 60          # Vertical mouth opening
width_factor = 0.7 + (frequency * 0.8)    # 0.7-1.5x horizontal stretch  
height_factor = 0.8 + (amplitude * 0.6)   # 0.8-1.4x vertical scaling
alpha = 0.9                        # 90% blending strength
scale_factor = np.clip(scale_factor, 0.6, 1.8)  # Transform limits
```

---

## Alternative Approaches Considered

### Option 1: Fix Current System (Quick Win)
**Approach**: Improve blending techniques to eliminate visible boundaries

**Implementation Ideas**:
- Gaussian blur masks for soft edges
- Feathered blending instead of hard rectangular boundaries  
- Multi-scale blending (blend at different resolutions)
- Distance-based alpha blending (fade effect from center outward)

**Estimated Effort**: 2-4 hours
**Expected Result**: Cleaner boundaries, still geometric-based animation

### Option 2: AI-Based Lip Sync (Wav2Lip) â­ RECOMMENDED
**Approach**: Use neural network trained on speech-to-video data

**Model**: Wav2Lip - State-of-the-art audio-driven lip sync
- Input: Audio + Face image
- Output: Realistic lip-synced video frames
- Pre-trained on massive datasets

**Pros**:
- ğŸ”¥ Hollywood-quality lip sync
- ğŸ”¥ No visible boundaries or artifacts  
- ğŸ”¥ Realistic mouth movements
- ğŸ”¥ Handles different speech patterns naturally

**Cons**:
- âš ï¸ Requires additional model download (~100MB)
- âš ï¸ More computationally intensive
- âš ï¸ May need GPU acceleration for real-time performance

**Files to modify**:
- `src/face_animator.py` - Add Wav2Lip integration
- `requirements.txt` - Add torch, torchvision dependencies
- New file: `src/wav2lip_animator.py`

### Option 3: Audio-Driven Face System Enhancement
**Approach**: Improve existing `src/audio_driven_face.py` system

**Current State**: Available but not actively used
**Potential**: More sophisticated than dlib but less than AI

### Option 4: Hybrid Approach
**Approach**: Combine multiple techniques
- Use AI for mouth region generation
- Use dlib for positioning and tracking
- Use audio analysis for timing

---

## Next Steps Decision Tree

```
Choose path:
â”œâ”€â”€ Quick Fix (2-4 hours)
â”‚   â””â”€â”€ Fix blending boundaries in current dlib system
â”‚   â””â”€â”€ Good: Fast, reliable
â”‚   â””â”€â”€ Bad: Still geometric, limited realism
â”‚
â”œâ”€â”€ AI Upgrade (1-2 days) â­ RECOMMENDED
â”‚   â””â”€â”€ Implement Wav2Lip integration  
â”‚   â””â”€â”€ Good: Professional quality, seamless
â”‚   â””â”€â”€ Bad: More complex, model download required
â”‚
â””â”€â”€ Alternative Enhancement (4-8 hours)
    â””â”€â”€ Enhance audio_driven_face.py system
    â””â”€â”€ Good: Middle ground approach
    â””â”€â”€ Bad: Unknown quality outcome
```

---

## Technical Architecture

### Current File Structure:
```
src/
â”œâ”€â”€ face_animator.py           # Main animation coordinator
â”œâ”€â”€ dlib_face_animator.py     # Current working system âœ…
â”œâ”€â”€ audio_driven_face.py      # Alternative system (underutilized)
â”œâ”€â”€ display_manager.py        # Screen output (working âœ…)
â”œâ”€â”€ audio_manager.py          # Audio I/O (working âœ…)
â””â”€â”€ speech_processor.py       # Speech analysis (working âœ…)
```

### Current Data Flow:
```
Audio Input â†’ Speech Processor â†’ Face Animator â†’ dlib system 
    â†“
Audio Analysis (amplitude, frequency) â†’ Mouth Landmark Manipulation
    â†“  
Geometric Transform â†’ Rectangular Blend â†’ Display Output
```

### Proposed AI Data Flow:
```
Audio Input + Base Face Image â†’ Wav2Lip Model â†’ Generated Frame
    â†“
Direct Display Output (no geometric transforms needed)
```

---

## Configuration Values (Current Working Settings)

### Display Settings (`config.py`):
```python
PROJECTOR_WIDTH = 1280
PROJECTOR_HEIGHT = 720  
FULLSCREEN = False
FACE_SCALE = 0.8
```

### Animation Intensity (`src/dlib_face_animator.py`):
```python
# These values are dialed in and working:
jaw_drop = amplitude * 60              # Mouth opening
width_factor = 0.7 + (frequency * 0.8) # Horizontal stretch
height_factor = 0.8 + (amplitude * 0.6) # Vertical scaling
audio_sensitivity = rms * 4.0           # Audio responsiveness
```

---

## Fallback Plan ğŸ›¡ï¸

If Wav2Lip implementation fails or performs poorly:

1. **Immediate fallback**: Current dlib system is working and committed
2. **Quick improvement**: Implement boundary blending fixes
3. **Alternative**: Enhance audio_driven_face.py system
4. **Last resort**: Tune current system parameters further

### Rollback Commands:
```bash
# If Wav2Lip branch fails, return to current working state:
git checkout master  
git log --oneline -10  # Find last working commit
```

**Last Known Working Commit**: `4b9280d` - "Dramatically increase mouth movement intensity"

---

## Success Metrics

### Current System (Baseline):
- âœ… Mouth moves in sync with speech
- âœ… No crashes or black boxes
- âœ… Full face visible and properly positioned
- âŒ Visible rectangular boundaries around mouth

### Target Success (Wav2Lip):
- âœ… All baseline achievements maintained
- âœ… No visible boundaries or artifacts
- âœ… Realistic, natural-looking lip movements
- âœ… Maintains real-time performance

---

## Development Notes

### Lessons Learned:
1. **Safety first**: Always validate transforms before applying
2. **Incremental approach**: Fix one issue at a time
3. **Commit frequently**: Each working state should be saved
4. **Parameter tuning**: Small changes can have big visual impact

### Common Pitfalls to Avoid:
- Don't apply transforms outside image boundaries
- Validate all array shapes before blending
- Always have fallback for failed operations
- Test with different audio volumes/frequencies

---

*Last Updated: December 2024*
*System Status: WORKING - Ready for AI upgrade experiment*